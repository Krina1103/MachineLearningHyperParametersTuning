{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f060ae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pandas import DataFrame, read_csv, to_numeric\n",
    "import pandas as pd\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ca09e34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = read_csv('diabetes_train.csv')\n",
    "df_test = read_csv('diabetes_test.csv')\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74781649",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(columns=df_train.columns[8])\n",
    "X_test = df_test.drop(columns=df_test.columns[8])\n",
    "y_train = df_train[df_train.columns[8]]\n",
    "y_test = df_test[df_test.columns[8]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7243eef",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "974b419d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "criterion gini splitter best max_depth 2 max_features 4 min_samples_leaf 1 Train Error is 0.2357 Test Error is 0.2101 \n",
      "criterion gini splitter best max_depth 2 max_features 4 min_samples_leaf 3 Train Error is 0.2851 Test Error is 0.3193 \n",
      "criterion gini splitter best max_depth 2 max_features 8 min_samples_leaf 1 Train Error is 0.2280 Test Error is 0.2269 \n",
      "criterion gini splitter best max_depth 2 max_features 8 min_samples_leaf 3 Train Error is 0.2280 Test Error is 0.2269 \n",
      "criterion gini splitter best max_depth 10 max_features 4 min_samples_leaf 1 Train Error is 0.0431 Test Error is 0.2773 \n",
      "criterion gini splitter best max_depth 10 max_features 4 min_samples_leaf 3 Train Error is 0.0971 Test Error is 0.2857 \n",
      "criterion gini splitter best max_depth 10 max_features 8 min_samples_leaf 1 Train Error is 0.0185 Test Error is 0.2857 \n",
      "criterion gini splitter best max_depth 10 max_features 8 min_samples_leaf 3 Train Error is 0.0724 Test Error is 0.3025 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def decision_tree_classifier(df_train,df_test,depth_k,features_i,leaf_j):\n",
    "    DT_classifier = DecisionTreeClassifier(criterion='gini',splitter='best',max_depth=depth_k, max_features=features_i, min_samples_leaf=leaf_j )\n",
    "    DT_classifier.fit(X_train, y_train)\n",
    "    y_train_pred = DT_classifier.predict(X_train)\n",
    "    y_test_pred = DT_classifier.predict(X_test)\n",
    "    print(\"criterion gini splitter best max_depth {} max_features {} min_samples_leaf {} Train Error is {} Test Error is {} \" .format(depth_k ,features_i, leaf_j, '%.4f' % (1 - metrics.accuracy_score(y_train, y_train_pred)), '%.4f' % (1 - metrics.accuracy_score(y_test, y_test_pred))))\n",
    "    \n",
    "depth = [2,10]\n",
    "features = [4,8]\n",
    "leaf = [1,3]\n",
    "\n",
    "for k in depth:\n",
    "    for i in features:\n",
    "        for j in leaf:\n",
    "            decision_tree_classifier(df_train, df_test, k, i, j)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79be04e",
   "metadata": {},
   "source": [
    "## MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5004cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation relu batch_size auto hidden_layer_sizes (10,) learning_rate constant alpha 0.001 Train Error is 0.2881 Test Error is 0.3529 \n",
      "activation relu batch_size auto hidden_layer_sizes (10,) learning_rate constant alpha 0.05 Train Error is 0.3374 Test Error is 0.4202 \n",
      "activation relu batch_size auto hidden_layer_sizes (10,) learning_rate adaptive alpha 0.001 Train Error is 0.2789 Test Error is 0.3193 \n",
      "activation relu batch_size auto hidden_layer_sizes (10,) learning_rate adaptive alpha 0.05 Train Error is 0.2989 Test Error is 0.3193 \n",
      "activation relu batch_size auto hidden_layer_sizes (50,) learning_rate constant alpha 0.001 Train Error is 0.2881 Test Error is 0.3613 \n",
      "activation relu batch_size auto hidden_layer_sizes (50,) learning_rate constant alpha 0.05 Train Error is 0.2743 Test Error is 0.3613 \n",
      "activation relu batch_size auto hidden_layer_sizes (50,) learning_rate adaptive alpha 0.001 Train Error is 0.2250 Test Error is 0.3277 \n",
      "activation relu batch_size auto hidden_layer_sizes (50,) learning_rate adaptive alpha 0.05 Train Error is 0.2650 Test Error is 0.3277 \n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def multilayerperceptron_classifier(df_train,df_test,hiddenlayer_k,learningrate_i,alpha_j):\n",
    "    mlp_classifier = MLPClassifier(activation='relu',batch_size='auto',hidden_layer_sizes=hiddenlayer_k, learning_rate=learningrate_i, alpha=alpha_j )\n",
    "    mlp_classifier.fit(X_train, y_train)\n",
    "    y_train_pred = mlp_classifier.predict(X_train)\n",
    "    y_test_pred = mlp_classifier.predict(X_test)\n",
    "    print(\"activation relu batch_size auto hidden_layer_sizes {} learning_rate {} alpha {} Train Error is {} Test Error is {} \" .format(hiddenlayer_k ,learningrate_i, alpha_j, '%.4f' % (1 - metrics.accuracy_score(y_train, y_train_pred)), '%.4f' % (1 - metrics.accuracy_score(y_test, y_test_pred))))\n",
    "    \n",
    "hiddenlayer = [(10, ), (50, )]\n",
    "learningrate = ['constant','adaptive']\n",
    "alpha = [0.001,0.05]\n",
    "\n",
    "for k in hiddenlayer:\n",
    "    for i in learningrate:\n",
    "        for j in alpha:\n",
    "            multilayerperceptron_classifier(df_train, df_test, k, i, j)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73be4447",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f20a4a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_features auto criterion entropy n_estimators 200 max_depth 10 min_samples_split 5 Train Error is:  0.0262 Test Error is:  0.2269 \n",
      "max_features auto criterion entropy n_estimators 200 max_depth 10 min_samples_split 10 Train Error is:  0.0678 Test Error is:  0.2437 \n",
      "max_features auto criterion entropy n_estimators 200 max_depth 20 min_samples_split 5 Train Error is:  0.0046 Test Error is:  0.2353 \n",
      "max_features auto criterion entropy n_estimators 200 max_depth 20 min_samples_split 10 Train Error is:  0.0478 Test Error is:  0.2437 \n",
      "max_features auto criterion entropy n_estimators 500 max_depth 10 min_samples_split 5 Train Error is:  0.0247 Test Error is:  0.2437 \n",
      "max_features auto criterion entropy n_estimators 500 max_depth 10 min_samples_split 10 Train Error is:  0.0555 Test Error is:  0.2353 \n",
      "max_features auto criterion entropy n_estimators 500 max_depth 20 min_samples_split 5 Train Error is:  0.0077 Test Error is:  0.2185 \n",
      "max_features auto criterion entropy n_estimators 500 max_depth 20 min_samples_split 10 Train Error is:  0.0462 Test Error is:  0.2185 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def random_forest_classifier(df_train,df_test,estimator_k,depth_i,split_j):\n",
    "    rf_classifier = RandomForestClassifier(max_features='auto',criterion='entropy',n_estimators=estimator_k, max_depth=depth_i, min_samples_split=split_j )\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "    y_train_pred = rf_classifier.predict(X_train)\n",
    "    y_test_pred = rf_classifier.predict(X_test)\n",
    "    print(\"max_features auto criterion entropy n_estimators {} max_depth {} min_samples_split {} Train Error is:  {} Test Error is:  {} \" .format(estimator_k ,depth_i, split_j, '%.4f' % (1 - metrics.accuracy_score(y_train, y_train_pred)), '%.4f' % (1 - metrics.accuracy_score(y_test, y_test_pred))))\n",
    "    \n",
    "estimator = [200,500]\n",
    "depth = [10,20]\n",
    "split = [5,10]\n",
    "\n",
    "for k in estimator:\n",
    "    for i in depth:\n",
    "        for j in split:\n",
    "            random_forest_classifier(df_train, df_test, k, i, j)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf3a4f9",
   "metadata": {},
   "source": [
    "## Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b22851e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_features auto loss deviance n_estimators 20 max_depth 5 min_samples_split 200 Train Error is 0.1695 Test Error is 0.2437 \n",
      "max_features auto loss deviance n_estimators 20 max_depth 5 min_samples_split 400 Train Error is 0.2034 Test Error is 0.2269 \n",
      "max_features auto loss deviance n_estimators 20 max_depth 10 min_samples_split 200 Train Error is 0.1726 Test Error is 0.2521 \n",
      "max_features auto loss deviance n_estimators 20 max_depth 10 min_samples_split 400 Train Error is 0.2034 Test Error is 0.2269 \n",
      "max_features auto loss deviance n_estimators 80 max_depth 5 min_samples_split 200 Train Error is 0.1109 Test Error is 0.2269 \n",
      "max_features auto loss deviance n_estimators 80 max_depth 5 min_samples_split 400 Train Error is 0.1572 Test Error is 0.2269 \n",
      "max_features auto loss deviance n_estimators 80 max_depth 10 min_samples_split 200 Train Error is 0.0616 Test Error is 0.2353 \n",
      "max_features auto loss deviance n_estimators 80 max_depth 10 min_samples_split 400 Train Error is 0.1402 Test Error is 0.2017 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "def gradient_boosting_classifier(df_train,df_test,estimator_k,depth_i,split_j):\n",
    "    gb_classifier = GradientBoostingClassifier(max_features='auto',loss='deviance',n_estimators=estimator_k, max_depth=depth_i, min_samples_split=split_j )\n",
    "    gb_classifier.fit(X_train, y_train)\n",
    "    y_train_pred = gb_classifier.predict(X_train)\n",
    "    y_test_pred = gb_classifier.predict(X_test)\n",
    "    print(\"max_features auto loss deviance n_estimators {} max_depth {} min_samples_split {} Train Error is {} Test Error is {} \" .format(estimator_k ,depth_i, split_j, '%.4f' % (1 - metrics.accuracy_score(y_train, y_train_pred)), '%.4f' % (1 - metrics.accuracy_score(y_test, y_test_pred))))\n",
    "    \n",
    "estimator = [20,80]\n",
    "depth = [5,10]\n",
    "split = [200,400]\n",
    "\n",
    "for k in estimator:\n",
    "    for i in depth:\n",
    "        for j in split:\n",
    "            gradient_boosting_classifier(df_train, df_test, k, i, j)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6e71e9",
   "metadata": {},
   "source": [
    "## Voting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5193c2e7",
   "metadata": {},
   "source": [
    "### Combine the classifiers with the best test error you produced in Q1..Q4 using VotingClassifier and measure the training and test error for each of the following cases:  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2313056",
   "metadata": {},
   "source": [
    "### give equal weight to each classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "011bf4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error is 0.1572\n",
      "Test Error is 0.2437\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(criterion='gini', max_depth=2, max_features=4, min_samples_leaf=1, splitter='best')\n",
    "mlp = MLPClassifier(activation='relu', batch_size='auto',hidden_layer_sizes=(10,), learning_rate='constant', alpha=0.05)\n",
    "rf = RandomForestClassifier(max_features='auto', criterion='entropy', n_estimators=500, max_depth=20, min_samples_split=5)\n",
    "gb = GradientBoostingClassifier(max_features='auto', loss='deviance',n_estimators=80, max_depth=10, min_samples_split=400 )\n",
    "\n",
    "ensemble_clf = VotingClassifier(estimators=[('dt',dt),('mlp',mlp),('rf',rf),('gb',gb)], voting='hard', weights=[0.25,0.25,0.25,0.25])\n",
    "ensemble_clf.fit(X_train,y_train)\n",
    "pred_train = ensemble_clf.predict(X_train)\n",
    "print(\"Training Error is \" '%.4f' % (1-accuracy_score(y_train,pred_train)))\n",
    "pred_test = ensemble_clf.predict(X_test)\n",
    "print(\"Test Error is \" '%.4f' % (1-accuracy_score(y_test,pred_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a14335",
   "metadata": {},
   "source": [
    "### give weight proportional to 1/(1+trainingerror) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b7cb967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error is 0.0678\n",
      "Test Error is 0.2101\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(criterion='gini', max_depth=2, max_features=4, min_samples_leaf=1, splitter='best')\n",
    "mlp = MLPClassifier(activation='relu', batch_size='auto',hidden_layer_sizes=(10,), learning_rate='constant', alpha=0.05)\n",
    "rf = RandomForestClassifier(max_features='auto', criterion='entropy', n_estimators=500, max_depth=20, min_samples_split=5)\n",
    "gb = GradientBoostingClassifier(max_features='auto', loss='deviance',n_estimators=80, max_depth=10, min_samples_split=400 )\n",
    "\n",
    "#Deciding weights based on Training Errors of best parameters\n",
    "dt_w = 1/(1+0.2542)\n",
    "mlp_w = 1/(1+0.2589)\n",
    "rf_w = 1/(1+0.0031)\n",
    "gb_w = 1/(1+0.1402)\n",
    "\n",
    "ensemble_clf = VotingClassifier(estimators=[('dt',dt),('mlp',mlp),('rf',rf),('gb',gb)], voting='hard', weights=[dt_w,mlp_w,rf_w,gb_w])\n",
    "ensemble_clf.fit(X_train,y_train)\n",
    "pred_train = ensemble_clf.predict(X_train)\n",
    "print(\"Training Error is \" '%.4f' % (1-accuracy_score(y_train,pred_train)))\n",
    "pred_test = ensemble_clf.predict(X_test)\n",
    "print(\"Test Error is \" '%.4f' % (1-accuracy_score(y_test,pred_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
